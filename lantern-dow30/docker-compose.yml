version: '3.8'
services:
  # PostgreSQL database for Airflow metadata
  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 5s

  # Redis for Celery task queue (if needed for scaling)
  redis:
    image: redis:7
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5

  # Airflow webserver
  airflow-webserver:
    image: apache/airflow:2.10.2-python3.11
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      # Database connection
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      
      # Core settings
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: 1
      
      # Web server settings
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
      AIRFLOW__WEBSERVER__RBAC: 'true'
      AIRFLOW__WEBSERVER__AUTHENTICATE: 'true'
      AIRFLOW__WEBSERVER__AUTH_BACKEND: airflow.auth.backends.password_auth
      
      # Logging
      AIRFLOW__LOGGING__LOGGING_LEVEL: INFO
      AIRFLOW__LOGGING__FAB_LOGGING_LEVEL: WARN
      
      # Security
      AIRFLOW__WEBSERVER__SECRET_KEY: 'lantern-dow30-secret-key-change-in-production'
      
      # Google Cloud Platform
      GOOGLE_APPLICATION_CREDENTIALS: /opt/airflow/gcp.json
      GCP_PROJECT: ${GCP_PROJECT:-your-gcp-project}
      GCP_LOCATION: ${GCP_LOCATION:-us-central1}
      GCS_BUCKET: ${GCS_BUCKET:-your-gcs-bucket}
      GCS_PREFIX: ${GCS_PREFIX:-dow30-data}
      DOCAI_PROCESSOR_ID: ${DOCAI_PROCESSOR_ID:-your-processor-id}
      DOCAI_PROCESSOR_LOCATION: ${DOCAI_PROCESSOR_LOCATION:-us}
      
      # Python path
      PYTHONPATH: /opt/airflow/lantern:/opt/airflow/dags
    volumes:
      - ./dags:/opt/airflow/dags:ro
      - ./lantern:/opt/airflow/lantern:ro
      - ./requirements.txt:/opt/airflow/requirements.txt:ro
      - ./gcp.json:/opt/airflow/gcp.json:ro
      - ./examples:/opt/airflow/examples:ro
      - ./docs:/opt/airflow/docs:ro
      - airflow_logs:/opt/airflow/logs
      - airflow_data:/opt/airflow/data
    ports:
      - "8080:8080"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    command: >
      bash -c "
      pip install --no-cache-dir -r /opt/airflow/requirements.txt &&
      airflow db init &&
      airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@lantern.com || true &&
      exec airflow webserver
      "

  # Airflow scheduler
  airflow-scheduler:
    image: apache/airflow:2.10.2-python3.11
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      # Database connection
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      
      # Core settings (same as webserver)
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: 1
      
      # Scheduler specific
      AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 60
      AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT: 'false'
      
      # Google Cloud Platform (same as webserver)
      GOOGLE_APPLICATION_CREDENTIALS: /opt/airflow/gcp.json
      GCP_PROJECT: ${GCP_PROJECT:-your-gcp-project}
      GCP_LOCATION: ${GCP_LOCATION:-us-central1}
      GCS_BUCKET: ${GCS_BUCKET:-your-gcs-bucket}
      GCS_PREFIX: ${GCS_PREFIX:-dow30-data}
      DOCAI_PROCESSOR_ID: ${DOCAI_PROCESSOR_ID:-your-processor-id}
      DOCAI_PROCESSOR_LOCATION: ${DOCAI_PROCESSOR_LOCATION:-us}
      
      # Python path
      PYTHONPATH: /opt/airflow/lantern:/opt/airflow/dags
    volumes:
      - ./dags:/opt/airflow/dags:ro
      - ./lantern:/opt/airflow/lantern:ro
      - ./requirements.txt:/opt/airflow/requirements.txt:ro
      - ./gcp.json:/opt/airflow/gcp.json:ro
      - ./examples:/opt/airflow/examples:ro
      - ./docs:/opt/airflow/docs:ro
      - airflow_logs:/opt/airflow/logs
      - airflow_data:/opt/airflow/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "airflow", "jobs", "check", "--job-type", "SchedulerJob", "--hostname", "$(hostname)"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    command: >
      bash -c "
      pip install --no-cache-dir -r /opt/airflow/requirements.txt &&
      exec airflow scheduler
      "
  # Optional: Airflow CLI for debugging and management
  airflow-cli:
    image: apache/airflow:2.10.2-python3.11
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      GOOGLE_APPLICATION_CREDENTIALS: /opt/airflow/gcp.json
      GCP_PROJECT: ${GCP_PROJECT:-your-gcp-project}
      GCP_LOCATION: ${GCP_LOCATION:-us-central1}
      GCS_BUCKET: ${GCS_BUCKET:-your-gcs-bucket}
      GCS_PREFIX: ${GCS_PREFIX:-dow30-data}
      PYTHONPATH: /opt/airflow/lantern:/opt/airflow/dags
    volumes:
      - ./dags:/opt/airflow/dags:ro
      - ./lantern:/opt/airflow/lantern:ro
      - ./requirements.txt:/opt/airflow/requirements.txt:ro
      - ./gcp.json:/opt/airflow/gcp.json:ro
      - ./examples:/opt/airflow/examples:ro
      - ./docs:/opt/airflow/docs:ro
      - airflow_logs:/opt/airflow/logs
      - airflow_data:/opt/airflow/data
    profiles:
      - debug
    command: >
      bash -c "
      pip install --no-cache-dir -r /opt/airflow/requirements.txt &&
      tail -f /dev/null
      "

volumes:
  postgres_data:
    driver: local
  airflow_logs:
    driver: local
  airflow_data:
    driver: local

networks:
  default:
    name: lantern-dow30-network